) %>%
mutate(use = 1) %>%
predict(
ccds_famm, newdata = .,
type = "terms", se.fit = TRUE)
ccds_famm_pred_obj <-
ccds_df[1:401,] %>%
mutate(use = 1) %>%
predict(
ccds_famm, newdata = .,
type = "terms", se.fit = TRUE)
pffr_coef_df =
tibble(
term = c("(Intercept)", "use"),
coef = tfd(t(ccds_famm_pred_obj$fit[,1:2]), arg = seconds),
se = tfd(t(ccds_famm_pred_obj$se.fit[,1:2]), arg = seconds)) %>%
mutate(coef = coef + c(coef(ccds_famm)[1], 0)) %>%
mutate(
ub = coef + 1.96 * se,
lb = coef - 1.96 * se)
ccds_famm_pred_obj$fit
pffr_coef_df =
tibble(
term = c("(Intercept)", "use"),
coef = tfd(t(ccds_famm_pred_obj$fit[1:2,]), arg = seconds),
se = tfd(t(ccds_famm_pred_obj$se.fit[1:2,]), arg = seconds)) %>%
mutate(coef = coef + c(coef(ccds_famm)[1], 0)) %>%
mutate(
ub = coef + 1.96 * se,
lb = coef - 1.96 * se)
ccds_famm_pred_obj$fit[1:2,]
ccds_famm_pred_obj$fit
pffr_coef_df =
tibble(
term = c("(Intercept)", "use"),
coef = tfd(t(ccds_famm_pred_obj$fit), arg = seconds),
se = tfd(t(ccds_famm_pred_obj$se.fit), arg = seconds)) %>%
mutate(coef = coef + c(coef(ccds_famm)[1], 0)) %>%
mutate(
ub = coef + 1.96 * se,
lb = coef - 1.96 * se)
pffr_coef_df =
tibble(
term = c("(Intercept)", "use"),
coef = tfd(t(ccds_famm_pred_obj$fit), arg = ccds_df[1:401,]$seconds),
se = tfd(t(ccds_famm_pred_obj$se.fit), arg = ccds_df[1:401,]$seconds)) %>%
mutate(coef = coef + c(coef(ccds_famm)[1], 0)) %>%
mutate(
ub = coef + 1.96 * se,
lb = coef - 1.96 * se)
lb = coef - 1.96 * se)
pffr_coef_df =
tibble(
term = c("(Intercept)", "use"),
coef = tfd(t(ccds_famm_pred_obj$fit), arg = as.numeric(ccds_df[1:401,]$seconds)),
se = tfd(t(ccds_famm_pred_obj$se.fit), arg = as.numeric(ccds_df[1:401,]$seconds))) %>%
mutate(coef = coef + c(coef(ccds_famm)[1], 0)) %>%
mutate(
ub = coef + 1.96 * se,
lb = coef - 1.96 * se)
pffr_coef_df =
tibble(
term = c("(Intercept)", "use"),
coef = tfd(t(unlist(ccds_famm_pred_obj$fit)), arg = as.numeric(ccds_df[1:401,]$seconds)),
se = tfd(t(unlist(ccds_famm_pred_obj$se.fit)), arg = as.numeric(ccds_df[1:401,]$seconds))) %>%
mutate(coef = coef + c(coef(ccds_famm)[1], 0)) %>%
mutate(
ub = coef + 1.96 * se,
lb = coef - 1.96 * se)
length(ccds_famm_pred_obj$fit)
t(unlist(ccds_famm_pred_obj$fit))
ccds_famm_pred_obj$fit
dim(ccds_famm_pred_obj$fit)
ccds_famm_pred_obj$fit$`s(yindex.vec`()
ccds_famm_pred_obj$fit$`ti(subject,yindex.vec`()
n <- 5
lambda <- 1
alpha <- 0.05
samples <- 50000
CI_1 <- function(n, alpha, mean){
CI_left <- (n * qchisq(alpha/2, df = 2 * n))/(2 * mean)
CI_right <- (n * qchisq(1-(alpha/2), df = 2 * n))/(2 * mean)
len_CI <- abs(CI_right - CI_left)
return(list(len_CI = len_CI, CI_left = CI_left, CI_right = CI_right))
}
CI_2 <- function(n, alpha, mean){
se <- mean / sqrt(n)
CI_left <- max(0, (1/mean) - qnorm(1 - alpha/2) * se)
CI_right <- (1/mean) + qnorm(1 - alpha/2) * se
len_CI <- abs(CI_right - CI_left)
return(list(len_CI = len_CI, CI_left = CI_left, CI_right = CI_right))
}
CI_3 <- function(n, alpha, mean){
log_lambda_hat <- -log(mean)
se <- 1 / sqrt(n)
CI_left <- exp(log_lambda_hat - qnorm(1 - alpha/2) * se)
CI_right <- exp(log_lambda_hat + qnorm(1 - alpha/2) * se)
len_CI <- abs(CI_right - CI_left)
return(list(len_CI = len_CI, CI_left = CI_left, CI_right = CI_right))
}
mode_CI <- function(n, alpha, mean, mode){
if(mode == 1){
return(CI_1(n, alpha, mean))
}else if(mode == 2){
return(CI_2(n, alpha, mean))
}else{
return(CI_3(n, alpha, mean))
}
}
CI_1 <- function(n, alpha, mean){
CI_left <- (n * qchisq(alpha/2, df = 2 * n))/(2 * mean)
CI_right <- (n * qchisq(1-(alpha/2), df = 2 * n))/(2 * mean)
len_CI <- abs(CI_right - CI_left)
return(list(len_CI = len_CI, CI_left = CI_left, CI_right = CI_right))
}
CI_2 <- function(n, alpha, mean){
lambda_hat <- 1 / mean
se <- lambda_hat / sqrt(n)
CI_left <- max(0, lambda_hat - qnorm(1 - alpha/2) * se)
CI_right <- lambda_hat + qnorm(1 - alpha/2) * se
len_CI <- abs(CI_right - CI_left)
return(list(len_CI = len_CI, CI_left = CI_left, CI_right = CI_right))
}
CI_3 <- function(n, alpha, mean){
log_lambda_hat <- -log(mean)
se <- 1 / sqrt(n)
CI_left <- exp(log_lambda_hat - qnorm(1 - alpha/2) * se)
CI_right <- exp(log_lambda_hat + qnorm(1 - alpha/2) * se)
len_CI <- abs(CI_right - CI_left)
return(list(len_CI = len_CI, CI_left = CI_left, CI_right = CI_right))
}
mode_CI <- function(n, alpha, mean, mode){
if(mode == 1){
return(CI_1(n, alpha, mean))
} else if(mode == 2){
return(CI_2(n, alpha, mean))
} else {
return(CI_3(n, alpha, mean))
}
}
sample_functions <- function(n, lambda, alpha, mode_max, samples){
count_CI_low <- rep(0, mode_max)
count_CI_high <- rep(0, mode_max)
length_CI <- rep(0, mode_max)
for(i in 1:samples){
sample <- rexp(n, rate = lambda)
sample_mean <- mean(sample)
for(mode in 1:mode_max){
ci_result <- mode_CI(n, alpha, sample_mean, mode)
if(ci_result$CI_left <= lambda && lambda <= ci_result$CI_right){
# CI contains the true value of lambda, do nothing
} else {
if(lambda < ci_result$CI_left){
count_CI_low[mode] <- count_CI_low[mode] + 1
}
if(lambda > ci_result$CI_right){
count_CI_high[mode] <- count_CI_high[mode] + 1
}
}
length_CI[mode] <- length_CI[mode] + ci_result$len_CI
}
}
avg_length <- length_CI / samples
lower_non_coverage_prob <- count_CI_low / samples
upper_non_coverage_prob <- count_CI_high / samples
return(list(
avg_length = avg_length,
lower_non_coverage_prob = lower_non_coverage_prob,
upper_non_coverage_prob = upper_non_coverage_prob
))
}
sample_functions(n, lambda, alpha, 3, samples)
samples <- 500000
sample_functions(n, lambda, alpha, 3, samples)
CI_1 <- function(n, alpha, mean){
CI_left <- ((1/n) * qchisq(alpha/2, df = 2 * n))/(2 * mean)
CI_right <- ((1/n) * qchisq(1-(alpha/2), df = 2 * n))/(2 * mean)
len_CI <- abs(CI_right - CI_left)
return(list(len_CI = len_CI, CI_left = CI_left, CI_right = CI_right))
}
CI_2 <- function(n, alpha, mean){
lambda_hat <- 1 / mean
se <- lambda_hat / sqrt(n)
CI_left <- max(0, lambda_hat - qnorm(1 - alpha/2) * se)
CI_right <- lambda_hat + qnorm(1 - alpha/2) * se
len_CI <- abs(CI_right - CI_left)
return(list(len_CI = len_CI, CI_left = CI_left, CI_right = CI_right))
}
CI_3 <- function(n, alpha, mean){
log_lambda_hat <- -log(mean)
se <- 1 / sqrt(n)
CI_left <- exp(log_lambda_hat - qnorm(1 - alpha/2) * se)
CI_right <- exp(log_lambda_hat + qnorm(1 - alpha/2) * se)
len_CI <- abs(CI_right - CI_left)
return(list(len_CI = len_CI, CI_left = CI_left, CI_right = CI_right))
}
mode_CI <- function(n, alpha, mean, mode){
if(mode == 1){
return(CI_1(n, alpha, mean))
} else if(mode == 2){
return(CI_2(n, alpha, mean))
} else {
return(CI_3(n, alpha, mean))
}
}
sample_functions <- function(n, lambda, alpha, mode_max, samples){
count_CI_low <- rep(0, mode_max)
count_CI_high <- rep(0, mode_max)
length_CI <- rep(0, mode_max)
for(i in 1:samples){
sample <- rexp(n, rate = lambda)
sample_mean <- mean(sample)
for(mode in 1:mode_max){
ci_result <- mode_CI(n, alpha, sample_mean, mode)
if(ci_result$CI_left <= lambda && lambda <= ci_result$CI_right){
# CI contains the true value of lambda, do nothing
} else {
if(lambda < ci_result$CI_left){
count_CI_low[mode] <- count_CI_low[mode] + 1
}
if(lambda > ci_result$CI_right){
count_CI_high[mode] <- count_CI_high[mode] + 1
}
}
length_CI[mode] <- length_CI[mode] + ci_result$len_CI
}
}
avg_length <- length_CI / samples
lower_non_coverage_prob <- count_CI_low / samples
upper_non_coverage_prob <- count_CI_high / samples
return(list(
avg_length = avg_length,
lower_non_coverage_prob = lower_non_coverage_prob,
upper_non_coverage_prob = upper_non_coverage_prob
))
}
sample_functions(n, lambda, alpha, 3, samples)
samples <- 50000
sample_functions(n, lambda, alpha, 3, samples)
data<- c(4,0,1,2,1,0,3)
mean <- mean(data)
s <- 6*var(data)/7
mean^2-(1.96*2*mean*s/7)
mean^2-(1.96*2*mean*sqrt(s)/7)
mean
mean^2-(1.96*2*mean*sqrt(s)/sqrt(7))
mean^2+(1.96*2*mean*sqrt(s)/sqrt(7))
knitr::opts_chunk$set(echo = TRUE)
here::i_am(
"FDA Report 1.Rmd"
)
library(tidyverse)
library(tidyverse)
library(refund)
library(tidyverse)
library(refund)
library(mgcv)
library(tictoc)
library(vbvs.concurrent)
library(viridis)
library(patchwork)
library(dplyr)
library(ggplot2)
library(tidyr)
load(here::here("data", "pupil.Rdata"))
head(pupil)
mean_mod = mgcv::gam(percent_change ~ s(seconds, k=30, bs="cr") +
s(seconds, by=use, k=30, bs = "cr"),
data = pupil, method = "REML")
resid_df = pupil %>%
filter(!is.na(percent_change)) %>%
select(id, seconds) %>%
mutate(resid = mean_mod$residuals) %>%
pivot_wider(names_from = seconds, values_from = resid, names_prefix = "resid.")
resid_mat = as.matrix(resid_df[,-1])
rownames(resid_mat) = resid_df$id
fpca_results = fpca.face(resid_mat, argvals = unique(pupil$seconds), knots = 15)
eigenfunctions <- as.data.frame(fpca_results$efunctions)
colnames(eigenfunctions) <- paste0("Phi", seq(1, fpca_results$npc))
eigenfunctions$seconds <- unique(pupil$seconds)
pupil = pupil %>% left_join(., eigenfunctions, by = "seconds") %>%
as_tibble() %>%
arrange(id, seconds) %>%
mutate(id = factor(id))
fosr_mod <- mgcv::bam(percent_change ~
s(seconds, k=30, bs="cr") +
s(seconds, by=use, k=30, bs = "cr") +
s(id, by = Phi1, bs="re") +
s(id, by = Phi2, bs="re")+
s(id, by = Phi3, bs="re") +
s(id, by = Phi4, bs="re"),
method = "fREML", data = pupil, discrete = TRUE)
summary(fosr_mod)
# put required data inputs into a dataframe
# need to have same names as covariates in model
s_pred = seq(0,1, length.out = 100)
df_pred = pupil %>%
filter(id == first(id))
# call predict.gam
coef_est = predict(fosr_mod, newdata = df_pred, type = "terms", se.fit = TRUE)
tibble(beta1 =  coef_est$fit[, 2],
seconds = df_pred$seconds,
se_beta1 = coef_est$se.fit[,2],
lower = beta1 - 1.96 * se_beta1,
upper = beta1 + 1.96 * se_beta1) %>%
ggplot(aes(seconds, beta1)) +
geom_line() +
geom_line(aes(y = lower), linetype = 2, color = "blue") +
geom_line(aes(y = upper), linetype = 2, color = "blue") +
geom_hline(yintercept = 0, linetype = 3, color = "red") +
theme_minimal()
s_pred <- unique(pupil$seconds)
# Get the design matrix associated with s_pred
lpmat <- predict(fosr_mod, newdata = df_pred, type = "lpmatrix")
# Get the column indices associated with the functional term
inx_beta <- which(grepl("s\\(seconds\\)\\.[0-9]+", dimnames(lpmat)[[2]]))
# Get the design matrix A associated with beta(s)
Bmat <- lpmat[, inx_beta]
# Get Var spline coefficients (beta_sp) associated with beta(u, s)
beta_sp <- coef(fosr_mod)[inx_beta]
Vbeta_sp <- vcov(fosr_mod)[inx_beta, inx_beta]
beta.hat <- coef_est$fit[,2]
se.beta.hat <- coef_est$se.fit[,2]
# Number of bootstrap samples (B)
nboot <- 1e4
# Set up container for bootstrap
beta_mat_boot <- matrix(NA, nboot, length(s_pred))
# Do the bootstrap
for (i in 1:nboot) {
beta_sp_i <- MASS::mvrnorm(n = 1, mu = beta_sp, Sigma = Vbeta_sp)
beta_mat_boot[i, ] <- Bmat %*% beta_sp_i
}
# Find the max statistic
dvec <- apply(beta_mat_boot, 1, function(x) max(abs(x - beta_hat) / se_beta_hat))
# Find the max statistic
dvec <- apply(beta_mat_boot, 1, function(x) max(abs(x - beta.hat) / se.beta.hat))
# Get 95% global confidence band
Z_global <- quantile(dvec, 0.95)
beta_hat_LB_global <- beta_hat - Z_global * se_beta_hat
beta_hat_LB_global <- beta.hat - Z_global * se.beta.hat
beta_hat_UB_global <- beta.hat + Z_global * se.beta.hat
tibble(beta1 =  coef_est$fit[, 2],
seconds = df_pred$seconds,
se_beta1 = coef_est$se.fit[,2],
lower = beta_hat_LB_global,
upper = beta_hat_UB_global) %>%
ggplot(aes(seconds, beta1)) +
geom_line() +
geom_line(aes(y = lower), linetype = 2, color = "blue") +
geom_line(aes(y = upper), linetype = 2, color = "blue") +
geom_hline(yintercept = 0, linetype = 3, color = "red") +
theme_minimal()
coef_est$fit[, 2]
head(coef_est$fit)
Z_global
head(beta_mat_boot)
dimnames(lpmat)[[2]]
# Get the column indices associated with the functional term
inx_beta <- which(grepl("s\\(seconds\\):use\\.[0-9]+", dimnames(lpmat)[[2]]))
# Get the design matrix A associated with beta(s)
Bmat <- lpmat[, inx_beta]
# Get Var spline coefficients (beta_sp) associated with beta(u, s)
beta_sp <- coef(fosr_mod)[inx_beta]
Vbeta_sp <- vcov(fosr_mod)[inx_beta, inx_beta]
beta.hat <- coef_est$fit[,2]
se.beta.hat <- coef_est$se.fit[,2]
# Number of bootstrap samples (B)
nboot <- 1e4
# Set up container for bootstrap
beta_mat_boot <- matrix(NA, nboot, length(s_pred))
# Do the bootstrap
for (i in 1:nboot) {
beta_sp_i <- MASS::mvrnorm(n = 1, mu = beta_sp, Sigma = Vbeta_sp)
beta_mat_boot[i, ] <- Bmat %*% beta_sp_i
}
# Find the max statistic
dvec <- apply(beta_mat_boot, 1, function(x) max(abs(x - beta.hat) / se.beta.hat))
# Get 95% global confidence band
Z_global <- quantile(dvec, 0.95)
Z_global
beta_hat_LB_global <- beta.hat - Z_global * se.beta.hat
beta_hat_UB_global <- beta.hat + Z_global * se.beta.hat
tibble(beta1 =  coef_est$fit[, 2],
seconds = df_pred$seconds,
se_beta1 = coef_est$se.fit[,2],
lower = beta_hat_LB_global,
upper = beta_hat_UB_global) %>%
ggplot(aes(seconds, beta1)) +
geom_line() +
geom_line(aes(y = lower), linetype = 2, color = "blue") +
geom_line(aes(y = upper), linetype = 2, color = "blue") +
geom_hline(yintercept = 0, linetype = 3, color = "red") +
theme_minimal()
library(patchwork)  # 用于合并图表
# 绘制图 1：全局置信区间 (CMA)
plot1 <- tibble(beta1 =  coef_est$fit[, 2],
seconds = df_pred$seconds,
se_beta1 = coef_est$se.fit[,2],
lower = beta_hat_LB_global,
upper = beta_hat_UB_global) %>%
ggplot(aes(seconds, beta1)) +
geom_line() +
geom_line(aes(y = lower), linetype = 2, color = "blue") +
geom_line(aes(y = upper), linetype = 2, color = "blue") +
geom_hline(yintercept = 0, linetype = 3, color = "red") +
theme_minimal() +
ggtitle("Global Confidence Interval (CMA)")
# 绘制图 2：点对点置信区间 (Pointwise)
plot2 <- tibble(beta1 =  coef_est$fit[, 2],
seconds = df_pred$seconds,
se_beta1 = coef_est$se.fit[,2],
lower = beta1 - 1.96 * se_beta1,
upper = beta1 + 1.96 * se_beta1) %>%
ggplot(aes(seconds, beta1)) +
geom_line() +
geom_line(aes(y = lower), linetype = 2, color = "blue") +
geom_line(aes(y = upper), linetype = 2, color = "blue") +
geom_hline(yintercept = 0, linetype = 3, color = "red") +
theme_minimal() +
ggtitle("Pointwise Confidence Interval")
# 合并两个图表
combined_plot <- plot1 + plot2  # 使用 patchwork 包合并
# 显示合并的图表
print(combined_plot)
global_df <- tibble(
beta1 =  coef_est$fit[, 2],
seconds = df_pred$seconds,
se_beta1 = coef_est$se.fit[,2],
lower = beta_hat_LB_global,
upper = beta_hat_UB_global,
type = "Global Confidence Interval (CMA)"
)
pointwise_df <- tibble(
beta1 =  coef_est$fit[, 2],
seconds = df_pred$seconds,
se_beta1 = coef_est$se.fit[,2],
lower = beta1 - 1.96 * se_beta1,
upper = beta1 + 1.96 * se_beta1,
type = "Pointwise Confidence Interval"
)
combined_df <- bind_rows(global_df, pointwise_df)
ggplot(combined_df, aes(x = seconds, y = beta1, color = type)) +
geom_line() +
geom_line(aes(y = lower), linetype = 2) +
geom_line(aes(y = upper), linetype = 2) +
geom_hline(yintercept = 0, linetype = 3, color = "red") +
theme_minimal() +
labs(title = "Comparison of Global vs. Pointwise Confidence Intervals",
y = "Beta1",
x = "Seconds",
color = "Interval Type")
here::i_am("climate_data_analysis.Rmd")
source(here::here("code","climate_functions.R"))
source(here::here("code","climate_functions.R"))
library(patchwork)
save = F
library(MASS)
library(cope)
# Load data
####
load(here::here("data","climate_data.dat"))
summer
attach(temp)
#Global variables.
####
#Confidence level for CoPE sets.
alpha_CoPE = 0.1
#Nominal expected false area ratio for FARE sets.
alpha_FARE = 0.05
#Number of realizations to produce in the MC simulations.
N=1000
#Defining the model.
####
# Mask on land
mask = mask * ifelse(orog>0, 1, NA)
#Define the t vectors and n.
ta = current
ta = ta - mean(ta)
tb = future
tb = tb - mean(tb)
na = length(ta)
nb = length(tb)
n = na + nb
#Define Design matrix X.
X1 = c(rep(0,na),rep(1,nb))
X2 = rep(1,n)
X3 = c(ta,rep(0,nb))
X4 = c(rep(0,na),tb)
X = cbind(X1,X2,X3,X4)
#Define the data Y.
Y = array(0,c(length(lon),length(lat),n))
for(j in 1:na) Y[,,j] = summer[,,j,1]
for(j in 1:nb) Y[,,na+j] = summer[,,j,2]
head(Y)
ls(temp)
View(orog)
knitr::opts_chunk$set(echo = TRUE)
here::i_am(
"FDA Report 1.Rmd"
)
load(here::here("data", "pupil.Rdata"))
unique(pupil$id)
length(unique(pupil$id))
packageVersion("ggplot2")
available.packages("ggplot2")
available.packages("ggplot")
setwd("D:/DATA_550")
getwd()
renv::activate()
renv::restore()
setwd("D:/DATA_550/midterm_project")
renv::restore()
